#!/bin/sh
#SBATCH --partition=amdgpuextralong
#SBATCH --time=504:00:00
#SBATCH --gres=gpu:1
#SBATCH --ntasks-per-node=1 # tasks per node
#SBATCH --mem-per-gpu=40000
#SBATCH --job-name=llama2_FT
#SBATCH --err=llama2_InA_20epoch.err
#SBATCH --out=llama2_InA_20epoch.out
#SBATCH --mail-user=kangchen@fel.cvut.cz    # where send info about job
#SBATCH --mail-type=ALL              # what to sen  d, valid type values are NONE, BEGIN, END, FAIL, REQUEUE, ALL

/bin/hostnamesbatch
srun -l /bin/hostname
srun -l /bin/pwd

ml Python/3.10.4-GCCcore-11.3.0
ml CUDA/11.7.0
source /home/kangchen/Chatbot/Psych_BioGPT/EnvLlama/bin/activate
cd /home/kangchen/Chatbot/chatbot-chinese/

CUDA_VISIBLE_DEVICES=0 python finetune/finetune.py \
--base_model /home/kangchen/Chatbot/Psych_BioGPT/models/input/llama/Llama-2-7B-fp16/ \
--model_type "llama" \
--data_dir datasets/psychtherapy_data.json \
--output_dir finetuned/llama-2-7B_psychtherapy_20Epochs_InA \
--num_epochs 50 \
--learning_rate 0.001 \
--batch_size 16 \
--micro_batch_size 16


# git lfs install
# git clone https://huggingface.co/WizardLM/WizardLM-7B-V1.0

