00:00:00
TRANSCRIPT OF VIDEO FILE: 

00:00:00
_____________________________________________________________________ 

00:00:00
BEGIN TRANSCRIPT: 

00:00:00
MICROTRAINING 

00:00:00
AN IMPRINT OF ALEXANDER STREET 

00:00:05
ALEXANDER STREET 

00:00:05
GREAT TEACHERS 

00:00:05
GREAT COURSES 

00:00:10
How Clinicians Can Take a Scientific Approach to Practice 

00:00:10
Presented By 

00:00:10
EVELYN BEHAR, Ph.D. 

00:00:10
UNIVERSITY OF ILLINOIS 

00:00:20
EVELYN BEHAR, Ph.D. 

00:00:20
ASSOCIATE PROFESSOR, UNIVERSITY OF ILLINOIS 

00:00:20
EVELYN BEHAR Hi, my name is Evelyn Behar, I am an Associate Professor at the University of Illinois at Chicago, in the department of Psychology. And today I am going to be talking a little bit about how clinicians… practicing clinicians can take a scientific approach to their practice. And umm… I am primarily psychotherapy outcome researcher. So, in my lab we… we look at sort of formal treatments in very tightly controlled environments to investigate what kind of treatment works best for a particular problem. But the truth is in reality most people are not researchers, they don't run large scale treatment outcome studies and instead, they just want to be able to take a little bit more of a scientific approach to being a clinical psychologist. So that's what we will be talking about today. And thank you all for coming. And we'll go ahead and get started. So, let's talk a little bit about the nature of umm… examining change in psychotherapy. There's sort of this ideal approach and then there's the more realistic approach. So the ideal approach is to run tightly, tightly controlled treatment outcome studies where you know, maybe you have a dismantling design or you have an additive design or you have a catalytic design or a parametric design that allows you to draw extremely strong causal conclusions. And you're taking care as the researcher to hold constant all factors across conditions except the one factor that you're interested in and you walk away seven years later, after you've run all of your participants through the study and you've done all of your coding and you've looked at everything and you did the follow up assessments, and used analyze the data and finally, finally got it published in a paper, and so seven years later you have your answer. But the reality of it is that obviously, these are very large studies. They take a very long time to run and they can be extraordinarily expensive, costing even tens of millions of dollars in federal grant money. And sometimes people are… investigators are interested in a particular problem that is actually not so common, right. So for example there's… I've been hearing a lot about this new condition called misophonia, and it's a sound sensitivity condition where individuals at the… at the certain sounds they start to experience real rage, like a real anger response, it's not an anxiety response, it's an anger response. And… umm… And there seem to be some similarities across individuals about what stimuli cause this. Nobody has ever run a treatment outcome study on misophonia before. And you know, maybe first to kind of want to learn a little bit about what treatments might be effective before you go and invest tons of money. You might not be able to find a lot of individuals who meet criteria, that aren't even criteria yet to meet because it's not seen as a formal diagnostic category in the DSM. And most importantly some professionals are not researchers. In fact most mental health professionals are not researchers, that's not what they want to do with their lives and that's totally okay. Instead, they want to be treatment providers, they don't… they're not interested in being researchers, they really want to provide treatment to suffering individuals. So they are clinicians. And they're not going to go and run a seven year longitudinal study in a tightly controlled laboratory, but that doesn't mean that they don't want to be scientific in their approach. Maybe they really, really want to be scientific in their approach and they just need alternative ways of doing that. And so, that's really what this lecture is focused on today, how to bring science into the therapy room as opposed to the laboratory. So, let's… I'm… I'm going to be presenting many different approaches to you and how to bring science into the therapy room. But let's start with the most, with the simplest one, the most straightforward one. The one that really any practicing clinician can do and it doesn't take a lot of resources, it doesn't take a lot of time, it really is just very simple. So, let's say that you have a client, right, and the person let's say, suffers from depression and anxiety, you know which tends to be the most common presenting problems for individuals who seek out therapy. So, you have somebody who is both depressed and anxious, and when the person first comes in, you just give them some self-report questionnaires, you might give them the Beck Depression Inventory, you might give them the state-trait anxiety inventory, you might give them the Penn State worry questionnaire, which measures worry symptoms. And importantly, maybe you also want to select some self-report measures that look at broader functioning, not just these specific symptoms, but a little bit more broad, something maybe like the dysfunctional attitude scale which was published by Aaron Beck, I think in 1979. The Sheehan disability scale that looks at overall impairment as a result of psychological distress. May be some health related measures, if you have umm… depressed client who is also umm… the… the depression is starting to impact their health. And importantly, you don't just want to pick any old measure out there. You want to make sure that the measure that the… the measures that you select are psychometrically strong, right. That they are valid, they're valid ways of measuring some of these constructs, that they're reliable, that if you administer umm… it to the person today that tomorrow the person is going to score in generally the same range. So, umm… and the reason for this is that you really want to make sure that you're doing a good job of measuring these issues and not just you know, measuring something that is not what you really intended to measure. So you administer these outcome measures to the client before therapy, maybe a few times throughout therapy and then again, after therapy. And maybe you want to bring them back three months later or send it… send them these measures over e-mail and say, hey, could you fill these out and send it back, so that you have some umm… indication of whether your treatment effects were maintained over time. And what you want to do is you want to measure the client's progress over time. And importantly, adjust the treatment when you don't see evidence of improvement, right. So umm… you know, I… I treat clients on the side and I mean, my… my primary job is as an academic, but I also do some clinical practice. And one thing is absolutely true and that is that we want to see our clients get better. And what do we know about human judgment and human decision making? We're biased… We are biased individuals and that is simply the nature of reality in terms of human cognition. So instead of me saying, oh, my client's is getting better. Absolutely. Because of course, I want to believe that, of course I'm going to think that because I'm… I'm invested in having been a good therapist. I should be a little bit more objective here and maybe just give the client some self-report questionnaires so that then I can rely on some objective data in order to tell me whether the client is getting better. Also, I don't want to have a client who for five weeks isn't getting better, right. Maybe the person got better and then plateaued(ph). Well, maybe now it's time to bring in a different type of treatment component as opposed to it just continuing to do the same thing that sort of stopped working. So, let's take a look at… this is actually a real case that I treated. So we'll call this individual David. And these are, on the Y-axis you have Beck Depression Inventory scores and over here you have sort of the agreed upon cutoffs in the literature. So from zero to 13 indicates minimal depression, 14 to 19 is an indicative of mild depression, 20 to 28 is moderate depression and 29 to 63 is severe depression. And I should say, David is one of the most severely depressed individuals I have ever encountered in my life. He… umm… he was just so… if there was apathy, there was incredible irritability, there was overwhelming sadness and really almost an inability to get out of bed. And he hadn't gotten out of bed in a long time, he has hygiene, his personal hygiene had really taken a turn for the worse. And thankfully, he actually did come into treatment. And I immediately just started doing behavior therapy as a behavioral activation because if you go into the literature, it shows, listen behavioral activation is sort of the first line of treatment. We know that it's going to be effective for the vast majority of people. And you'll see that at baseline his BDI scores, Beck Depression Inventory score was quite high, it was at about a 61, 62, which is really, I mean, on a scale of zero to 63, that's about as bad as it gets. And… And I remember, he was filling out the BDI and he was like, I don't really care, like, just… and I said just give it a try, just do this one thing. And so he did it, and he maxed out. Okay, so from baseline all the way up to about session eight David went from a 61 to about a 30. That was huge. I mean, that is an enormous improvement right, just from behavioral activation, that was it. Now what's interesting is that there's a literature that suggests, I believe it's Larry Buhler is the person who published this. It suggests that the majority of change in psychotherapy will take place in the first eight sessions, that that's where you'll see the biggest slope in change. And then after that people will continue to get better often, but it's not at the same degree of that slope. So I continued doing behavioral activation with… with David, but look at what happened, there's was a plateau, right. He's really just not getting better. And I was administering the beck Depression Inventory every two sessions. Why? Because the BDI asks about symptoms over the past two weeks. So it doesn't really make sense to administer it every session, I did it every two sessions instead. And I noticed that between session eight and session 16, you know, nothing changed. And I thought to myself, all right, if I just continue doing the same thing I'm not doing him any favors here. I am not doing him any favors at all. And so at session 16 I… I presented the Cognitive Therapy Rationale to him and he had a very negative reaction, he didn't like it. And he didn't like the idea that maybe umm… his thinking was somehow flawed, maybe I didn't sell it so well. But I thought you know, what, I'm not going to push it on him. So I went into the literature and I saw this wonderful umm… literature, largely by Les Greenberg in Canada on emotion focus therapy for depression. And I thought, you know, I think David is going to really benefit from this. So at session 16, I started doing Emotion Processing… Emotional Processing Therapy and I continued by the way, with the behavioral activation. I told him, don't stop, we're going to continue doing that, but we're going to spend 10 minutes of every session talking about behavior activation and they're going to segue to the next 50 minutes on emotional processing. And between sessions 16 and 30, you see that David went down to a BDI, I think his ending BDI was something like a 17 or so, which indicates mild depression. So, from the baseline at session eight, sure, he got here, but you know what, that still here depression in the 30s. So he was better, but it wasn't… it's not like I cured the guy, it's not like, you know, it's not like you can now walk away and be functioning at the level of the general population, he still had some work to do. But it was only by measuring change, so I was able to see, hey, he got better, and then he stopped getting better. And so something different has to happen. So, this is the most simple approach to being scientific as a practicing clinician is to just measure change and then be honest with yourself and say, you know what, what I've been doing it's not getting the person incrementally better, now I've got to switch. Okay. So here are the pros of the simple approach. I'm calling it the simple approach, it's really not that simple, but it's simple in its implementation. It allows the clinician to show objective evidence of improvement, right, not relying on his or her own gut feeling, but rather this nice objective evidence. You can measure multiple domains of functioning. So umm… you know, you can… you can really look and see like, is it just the depression that's getting better or are the insomnia symptoms also getting better, is his health getting better, etcetera, etcetera. And if those broader symptoms are not being targeted maybe you can, maybe the clinician can tweak the treatment a little bit in order to try to… to… to target those broader symptoms. And this really is superior to what the vast majority of practicing clinicians do, the vast majority of practicing clinicians don't hand over a self-report questionnaire in order to measure change. It's… It's actually to me, it's… it's remarkable that that's what happens. You know, when you go to an MD, they do blood tests to see if your cholesterol is getting better, right. Why should practicing clinicians not do the equivalent, which is to check in about the core symptom that… that is being targeted. And also, it can be a motivating factor for the client. So if you just keep a little Excel spreadsheet and then you show the client how much they've improved over the course of the last eight weeks, that person can really find a lot of motivation from seeing objective evidence of his or her improvement. And it can really get them to agree to continue the treatment, may be ramp it up a little bit you know, up the ante a little bit in order to get even greater change. Here are the cons, the cons are that there are lots of threats here to internal validity, and here's the major one. You know, while you're treating this person time is also passing, and we know that when time passes, people get better especially, with depression. So depression tends to have sort of this, umm… like a… a temporally… it increases and then decreases, it increases and then decreases, increases, decreases. And so how do I know that David getting better was because of the therapy and not just because time was passing and maybe he was going to feel better even if he hadn't come to see me. Also there is such a thing a spontaneous remission. You know, people… there are a lot of people who have a very severe onset of symptoms and then it just goes away. Somehow it magically goes away. And it doesn't control for anything, it's not measuring spontaneous remission. How do I know that it wouldn't have gotten better anyway? And also you have to remember that there are non-specific treatment elements here. I'm not only doing behavioral activation and emotion focus therapy, I am meeting with David and I'm giving him a place to go where he is meeting with somebody who cares about him and who is paying attention to him, and his concern about him. And when he expresses suicidality, I call him a few days later just to make sure he's okay, and that he is not actively trying to harm himself. And all of those things are clearly going to make David feel better. Maybe it has nothing to do with the behavioral activation at all and I can't rule that out, I can't rule out that rival hypothesis. So, if what I want to do as a treatment provider, as a clinician is maybe to be a little bit more scientific in my approach and maybe try to rule out some of those rival hypotheses that have to do with the passage of time. There is a way of doing it and they're small-N designs, and I want to just preface this by saying that small-N designs, you have to be careful because they're not real experiments, they don't give you that… that great satisfaction in the pit of your stomach that tells you I know that there is cause-and-effect here, the way that you do with real treatment outcome studies that are tightly controlled. There's always the possibility that change was going to happen anyway. But what the small-N designs do is it just increases your confidence a tiny bit and you'll see how that's the case. It's also the case that small-N designs suffer from some pretty big limitations which we'll get to, and we have to be honest about what those limitations are. But I would still argue they're better than nothing or better than not being scientific in your approach to doing therapy. Okay. So in general, in small-N experimental designs you're making causal inferences, not causal conclusions, they are causal inferences about the effectiveness of a treatment based on presenting different conditions to the same client over time and tracking that client's progress. It's almost like an experiment with an N of one, just one person. They require many observations either daily or weekly. And one thing to keep in mind, I'm going to go over three different small-N experimental designs with you. It's important to examine the pattern and stability of the behavior during each phase of the "experiment". And I'm going to put experiment in quotes because it's not a trick experiment, you don't have random assignment. Okay, the three types that we're going to learn about are the ABAB design, the multiple baseline design and this has many different applications which we'll get into and the changing-criterion design. So let's start with the ABAB design. So here, on the Y-axis you have the dependent variable here is number of tantrums per day. So let's say that you're treating this little boy, his name is Billy and Billy is having a real problem with temper tantrums and his parents are really having a hard time controlling them, they're getting out of hand and Billy's mom brings him in. And she says, look, I can't… I can't take this anymore. I… We've got to get the tantrums under control, our house has become a war zone. And you say, Mrs. Billy's mother, no worries, we're just going to do behavior modification with your child. It works like a charm, it works for the vast majority of children, if in the end it doesn't work, we'll pick something else, but let's start with behavior modification. And she says, great. So what you want to do is just take an initial… you want to see that Billy's… you want to see what Billy's behavior number of tantrums is at baseline, right. So, A always represents baseline in the ABAB design. And here, Billy is having about 15 tantrums per day, this poor mother. And now, so between sessions zero and one, you establish that Billy is having 15 tantrums per day and it's stable. Importantly, it's stable over the course of the week. Those seven days of that first week you see maybe 15, 14, 16, 13, but it's all hovering around there. If it's varying widely then you've got a problem,, and you really need to wait until you get a stable… stable read for reasons that I'll explain. Now you implement behavior modification and you see what happens to the tantrums. And obviously this is a… this is an idealized outcome. You see what happens to the tantrums, they're going down to two tantrums a day. Okay. Now if we just stop there we say to ourselves, but how do I know the tantrums wouldn't have gone away anyway, right. Maybe Billy's parents were fighting a lot and now they stop fighting and so that's why the tantrums went down, not because of behavior modification but because this other thing was happening. Or maybe Billy had a gluten allergy and they figured it out and now they're not feeding him gluten and that's why the tantrums have decreased, not because of behavior modification. That would be the primary threat to internal validity if we were just doing this treatment you know, without an ABAB design. In an ABAB design what you do is you come back to A, you take away the treatment and you say, all right, let's go back to baseline, no treatment, let's see what happens to the number of tensions per day. Here it goes back up to 14. Say, huh, so when I take away the treatment the behavior goes back up, when I implement the treatment it goes that down. Let me go back and forth, back and forth, and you see that the… in this idealized outcome you see that the… the behavior, the target behavior follows the intervention, either the delivery of the intervention or the withdrawal of the intervention. So if you see this as a clinician you can say to yourself, gosh, it's probably the treatment, right. Because probably there… if… if the gluten allergy theory were true, I am guessing that they're not giving him gluten here and not giving him gluten here, they probably just eliminated gluten altogether. So that's not really the rival hypothesis. So, again, we cannot draw strong causal conclusions and say, absolutely there's nothing else happening, it's the behavior modification that's causing the improvement in number of tantrums per day. We can't say that because it's not a true experiment, but we can make this causal inference, we can say you know, I'm somewhat confident in the idea that it's actually the… the intervention that's causing a change in this behavior. That's the ABAB design. All right, what are the pros of the ABAB design? Well, it's nice that you can infer that the treatment is causing a change in the behavior, right. It's better than your typical treatment with no ABAB design where you don't know if it's the treatment or not that caused change. But there are some very serious cons here and I'm hoping that you are already picking up on what some of these are. Notice, that in order to make these causal inferences the behavior has to revert back to its old ways, right. You've got here, it's got to go back up. And there are three issues. The clinical issue is when you have a good treatment, the… the effects last. Behavior modification is a great treatment. And when you implemented it the effects don't just go away because you took away the treatment, the effects should last over time. There's also a very serious ethical issue. If something is working for a suffering child and a suffering family, are you really going to be the clinician to take it away? I don't want to be the clinician to take it away, there's a serious ethical problem there with umm… with taking away something that you know is causing a decrease in suffering. And you certainly don't want to be the clinician who does that. However, if you don't do it, right, and the client's behavior remains improved then you've got a scientific issue which is to say, like, I can't make any causal inferences here, I don't know if this would have remained improved or not. There's really no way to know. So it's a tradeoff. You have to take into consideration the clinical… realistic clinical and ethical umm… limitations together with this scientific consideration and… and it's a tradeoff. Like, you're certainly aren't going to make a child suffer just to be able to draw a causal inference. If it worked, it worked and you're happy as a clinician that it worked and his parents are happy and he's happy. But if you do want to take sort of this scientific approach, this is something that you might try doing. So I'll give you an example, I actually treated a woman once who came in with a problem I had never encountered. And that was that she would randomly burst into tears, umm… severe sobbing episode, 30 times a day, no precipitant that she could think of, no thought, no preceding cognition that she could think of, it just happened. And she had gone and gotten, you know hormonal medical work up with doctor just to make sure that everything was okay, everything was fine. I thought, gosh, this is really weird, I've never encountered this before. I consulted to a bunch of other clinicians, nobody had any good advice for me. So I decided to do a paradoxical intervention and I said to her, try to cry, right now, go ahead try to cry. And she couldn't do it. So I said, okay, for the next week try to cry, every time you feel that little pit in your stomach that tells you, oh, oh, it's going to start, go ahead and put yourself into it, and try to cry, and then let's see what happens. So she went, went down to zero, no crying, a couple of little crying episodes, but really quite low. And I said, well, you know, I have to know, I have to know if it's the treatment or if this was just spontaneous remission. I said, okay, we'll call her Ethel. I said, all right, Ethel, for the next week, forget the trying to cry, just go back, go back to the way that you were doing it, try not to cry, try to suppress it. She went back it went all the way back up to 30 times a day. And so I thought, gosh, you know, I'm happy that I know, I'm happy that I can draw this causal inference that it was the treatment that worked and not… it wasn't just some spontaneous remission or passage of time, etcetera, etcetera. And the truth is, ethically, I didn't feel so bad because there was no change from what she was going through before, she had suffered for 20 years with it and… and I asked her permission, I said, are you okay, sort of withdrawing this treatment for the next week. I just… I feel like we need to know if it's the treatment or if this would have just gone away on its own. And she said, Evelyn, it wasn't gonna go away on its own, but I'll do it for you. So she went ahead and did it. And so we really walked away with something that actually looked a lot like this. And I felt very satisfied knowing that umm… or having some confidence in the idea that it was the treatment that caused the change. Okay. So, that's the ABAB design. Let's look at another one, this is the multiple baseline design. So I'm going to describe it and it's going to be a little bit confusing, but the examples will make it clear. So, here you collect baseline data on two or more baseline targets and the treatment is applied to each target at different times. So there are three versions of the multiple baseline design depending on what the targets are. In one version you treat different behaviors in the same client where the target is the behaviors. In another version you treat the same behavior among different clients, here the target is the clients themselves. And in the third version you treat the same behavior within the same client across different situations. And they're the target is the situations. I know that that's a little confusing, just bear with me. So the first thing that you do is you make sure that the baseline behavior is stable. You absolutely need stability from the beginning so that you know that this behavior isn't just wildly varying, that there are some stability to it. And then you implement your treatment for only one of the targets. That one target should improve while the others remain stable and then you implement your treatment for the next target and so on and so on. And the causal inferences here are… are… are realistic if each target changes only when the intervention is implemented and fails to change prior to the interventions. Let me give you an example, let's take our friend Billy and let's say that he has four problematic behaviors at home. He's wetting the bed, he's refusing to eat, he's hitting his brother and he's hitting himself. We have four target behaviors. So here, we're going to do the version of the multiple baseline design where the target is the actual behaviors. And we know that all of these behaviors can be treated with behavior modification. So from weeks one to three, we're just going to look at baseline data and we're going to wait until they're stable so that we know that there's some stability of behavior. In weeks four to six, we're going to do behavior modification only for the first one, we're going to leave the other three alone. Then, week seven to nine, we're going to behavior modification for the second one, we're going to leave the other two alone, then for the third and for the fourth. Okay. So, this is what in an ideal world it should look like. We have weeks one to three, right. So, the Y-axis is the frequency of behaviors each week. From weeks one to three we're just collecting baseline. So each line, each color line represents a different problematic behavior. Remember, the target here is the behaviors. So you see that all of these behaviors are fairly high, right. The incidence of them is fairly high and they're relatively stable, they're not going like this, they're relatively stable. Okay, starting at week four, now you're only going to treat one of the problems. Let's start with bedwetting. So that's the dark blue line. And you're going to leave the other three alone. So what should happen if there… if you can make causal inferences is that the incidence of bedwetting should decrease while the others remain high. They don't change along with the bedwetting, okay. And then you can say, hey, it's probably the case that the behavior modification worked for bedwetting because if it was just that Billy stopped eating gluten and so all of his problems went away, everything should decrease, not just that one, right. So, you're seeing specificity of treatment effects here. Now you're done with the bedwetting. So now you're going to say, all right, so starting… now I feel some degree of confidence that it was the treatment that caused the decrease in bedwetting. So now starting at week seven, after three weeks of behavior modification for bedwetting, I'm going to start looking at behavior modification for the refusal to eat, that's the pink line. So now starting at week seven, oh, the pink line goes down. We still have bedwetting low, right, because we took care of that problem, pink line starts to go down, but the other two remain high. Still, specificity of treatment effects, you're targeting that behavior only and that's the only one that decreases, the other two remain stable. Now we say, all right, three weeks of targeting eating… eating pathology, now we're going to do the yellow line, hitting his brother. We're going to behavior modification for hitting as brother, starting here. Oh, it goes down, but the other one, the hitting of himself remains stable. And then finally we go ahead and starting in the 13th week we start to treat Billy for hitting himself and we see that that behavior decreases. So you can see here that there's some degree of confidence in the specificity of the intervention, right. If it was just the passage of time, if Billy was just going to get better anyway then it wouldn't have been so specific to the actual intervention, it wouldn't have… that the targets, the behaviors wouldn't have followed the interventions so beautifully. Still possible, it's not a true experiment, you can absolutely claim cause-and-effect, but you do have a little bit more confidence in the idea that it's the treatment that's actually causing the improvement. Okay. You can also have a version of the multiple baseline design where the target is the client's, not the behaviors. So here, let's say that you have a small private practice and you specialize in the treatment of sleep disorders. And you treat insomnia, you treat delayed sleep phase disorder, you treat all sorts of things, but you've never treated nightmares. And all of a sudden you get four referrals and these four people come in and they're all suffering from nightmares and you look through the literature and you say, gosh, there's no widely accepted treatment for nightmares, what do I do? Well, maybe I'll try relaxation therapy, right. But you got to be careful, if you just give them all relaxation therapy and they all get better, well, maybe they were going to get better anyway. You have too many threats to internal validity there. So you're going to do the multiple baseline design. Again, weeks one to three, you're going to just examine baseline data, hopefully they'll be stable, if not, you've got to wait. Weeks four to six, you're going to do relaxation therapy only for Client No. 1, you're going to leave Clients 2, 3 and 4 alone. When that person gets better, then you're going to shift over to Client 2, you're going to leave Clients 3 and 4 alone. When that person gets better, you're going to shift over to relaxation therapy for Client No. 3. Hopefully, that person will get better and then finally for Client No. 4. So let's look at some made up data. Okay, so here you see that the targets are the clients, not the behavior, because it's the same behavior, you just have different individuals. Okay. So, the dependent variable is number of nightmares per week, right. And you see that in the beginning in those first three weeks you've got you know, this yellow person, that person is vacillating a little bit, but not by much, right. He's going between seven and five, it's fine. Everybody looks pretty stable. Okay. So starting at week four, you're only going to do relaxation therapy for the first client, that's the blue line. Oh, the blue… the blue client decreases in the number of nightmares per… per night, the other three are remaining stable. This allows you to say to yourself, ah, maybe it is the relaxation therapy because if it were just spontaneous remission maybe everybody would get better or if there you know, all of a sudden the riots have stopped occurring in the city and therefore people are sleeping more soundly, everybody would sleep more soundly, not just your one client. But you know, maybe the blue client was going to have spontaneous remission. Let's keep going. Now, you do that for three weeks with the first client, now in week seven you say, okay, I am going to start now with my next target, Client No. 2, that's the red one. Oh, Client No. 2 decreases. Isn't decreasing quite as much as Client No. 1, but you do see a decrease, with the other two clients remaining stable. You do that for three weeks and then you say, okay, let me give it a shot. Now, week no. 10, I'm just going to treat my yellow client. Now all of a sudden the yellow client decreases, and the green client has remained the same, the green client remains the only untouched client thus far. Then finally, in week 13 you say, okay, now I'm going to target my green client, green client goes ahead and decreases until the end where you have everybody low, right. So here, whereas in the first example with Billy, the target there was the actual behaviors. You have four different behaviors, here the targets are the clients. And you can see that the clients follow the treatment. The clients don't get better until you give each one the treatment. Okay, here's the third example, the third situation in a multiple baseline design. Here the target is the situations. So, let's say that you get a client with trichotillomania, chronic hair pulling, right, compulsive hair pulling. And umm… And the client says to you, you know, the hair pulling takes place in primarily four places in my life. It could be six, it could be anything, but we're just going to use four. It happens the most when I'm at my desk, when I'm in bed, when I'm in my car driving or when I'm just sitting on the sofa, watching TV. Those are the four places where it happens the most. And so you decide, okay, you go into the literature and you see, habit reversal training, that's what you're supposed to do for trichotillomania. Let's have a reversal training. Well, if the habit is to go ahead and pull the hair, pull the hair, then instead, you want to implement some sort of new motor(ph) behavior that's inconsistent with hair pulling. So you might have the client do something like this, right. You can't do this and pull your hair and its… in fact it's like the complete opposite of pulling your hair or sit on her hand or something that is really going to start to reverse that habit. And it's been shown to be pretty effective for trichotillomania. Okay, so you're going to do the exact same thing that you did in the other two examples. Weeks one to three, you're going to collect baseline data. Hopefully you'll see some stability of the behavior, if not, you've got to wait until it's stable, otherwise if it fluctuates too much, it's impossible to draw any causal inferences. Weeks four to six, you're going to habit reversal training only at her desk. You're going to leave the bed, the car and the sofa alone. You're only going to target that one situation. Week seven to nine, you're going to now target to do have a reversal training when she's in bed, but leave the other two alone. Weeks 10 to 12, you're going to do habit reversal training in the car, but leave the sofa alone. And finally, in weeks 13 to 15, you're going to do habit reversal training on the sofa and see if the… if the behaviors follow the habit… habit reversal training for those specific situations. So, let's look at our beautiful perfect made up data. And here you have again, on the Y-axis, you have the number of hairs pulled per day. So you see that all four clients are showing actually quite high levels of hair pulling, 50 hairs per day, coming out of… that had being pulled out of the head. So, starting at week four and thank goodness, you see it's nice and stable. So, I'm going to now implement my habit reversal training, but I'm going to do it only at the persons desk. You implemented at the desk, you see, wow, it really decreased all the way down to 19 or so and the others remain high and stable. So eventually at week seven, you say, all right, let's go ahead and take the second target, the bed. You see, oh, my gosh all of a sudden based on that habit reversal training that behavior is decreasing, the hair pulling is seriously decreasing, where the other to remain stable. Finally, in week 10, you implement habit reversal training when she's in the car, you see that, that comes down while the other one remains stable. And then finally in weeks 13, you say, all right, now let's go ahead and generalize it to the sofa when you're just sitting there, watching TV and you see that, that decreases. So again, you can't draw strong, strong causal conclusions, you don't have an… a true experiment with a lot of people, but you do kind of feel pretty good about the idea that it's probably the treatment that's causing these decreases in behavior because if not, if it were the passage of time, spontaneous remission whatever else, all of these would have decreased at once, not just one at a time. Okay. Of course there are some concerns here, right. So first of all, we have some clinical concerns. So, when the behaviors are the target, remember, the Billy example, right. The eating and the hitting and… good treatments generalize to other behaviors, they're not so specific. It's not like, okay, I'm going to teach Billy to not hit his brother, but he's still going to hit himself, he's probably also going to stop hitting himself so much, right. That's probably, realistically clinically what's going to happen. But the strength of your causal inferences almost depends on Billy not on… on… on Billy's improvement not generalizing to other behaviors. When the situations are the target, so that's the trichotillomania example, good treatments generalize to other conditions. Let's say, your client is all of a sudden going to not be pulling hair at all at her desk, but she's still going to be pulling 50 hairs a day in her car, probably not, probably not. It's probably going to generalize to some other situations. And when clients are the targets, remember, the four clients with the nightmares. It's potentially unethical to withhold the treatment that you think is going to work and that you have good reason to believe is going to work from people who are suffering all because you want the satisfaction of thinking that it was the treatment that worked. So again, it's going to be a tradeoff, right, in terms of when the… when the clients are the target, you do kind of want to know is that the treatment or was it just the passage of time and you almost have some level of obligation to know. But at the same time you don't want to withhold a treatment, that is working for one person from three other people. You don't want to do that. And the other one, this is just a clinical reality. I put up graphs that were perfect because I made up the data. But in clinical reality, good treatments have generalizable effects and we know that that's the truth. And overall, the scientific concern is that if the treatment generalizes to other target behaviors and situations then your causal inferences are just not going to be as strong. So the reality of it is you can do it and you can draw some causal inferences, but they might not be as strong as we were able to conclude from these graphs. Okay, then we have the final ones. So we had the ABAB design, we have the multiple baseline design and now we have the changing criterion design. So here, as with the other ones, first you take a baseline assessment and you wait until the behaviors are stable. Then, you implement a treatment and you decide on a criterion, some level of performance that you want to see. Once the client meets that criteria and/or that goal then you make the next goal little bit more stringent. When the client meets that goal, you make it a little bit more stringent. And now what you start doing is you start making bi-directional changes in the criterion. So you make it a little bit more stringent, a little bit less stringent, a little bit more stringent, a little bit less stringent. And you want to see that the behavior is following the criterion. That's what will allow you to make some causal inferences. So, let's take a look. Let's say that Molly's parents are having some trouble potty training her, right. And you say, oh, this is going to be easy, we're just going to do a little behavior therapy, we're going to do a star chart. Molly is going to be potty trained in no time at all. So, you do a baseline, you see that Molly is using the potty zero times per day, you implement a criterion and you say, all right, when Molly uses the potty once per day, she's going to get a prize at the end of the day, you know, whatever it is. And when Molly meets that criterion, then the next week you say, all right, now the new criterion is two times a day. So now Molly starts to use the potty twice a day and you say, huh, she wasn't using it twice a day when we just said once, her behavior seems to be following the criterion, but let's keep going. Then in the third week you say, all right, Molly, now it's three times per day, you have to use the potty in order to get the prize at the end of the day and you see that she does. Now you say, well, maybe Molly would have just kept on getting better, right. Maybe it's not the treatment, maybe Molly is just maturing and she's starting to have a little bit more control over her physiological states. So now you say, well, I want to know if it's the treatment. So you say, all right, Molly, now it's just once per day. Now if the behavior follows the criterion then you can make some causal inference, right. You can say, ah-ha, it's the… it's the treatment, it's not just that Molly seems to be gaining more control over her bladder, it really does seems to be the treatment. And then when we bump it up to four times a day, Molly follows the new criterion and she goes up to four times a day. And then we say, okay, now we're going to make another directional change, we're going to go down to two and Molly comes in and was able to achieve that twice per day. So you might see something that looks a little bit like this over time in terms of the… the Y-axis here is number of potty uses per day in Molly. And you'll see that you get the bi-directional changes as you change the criterion. All right. Obviously there are some concerns here too, right. So in order to make those causal inferences, the behavior has to revert back to its old ways when you begin to relax those criteria and what those goals are for your little client and for her parents and you have first of all, clinical concern. Again, good treatments have lasting effects. If Molly is able to use the potty three times per day then when you say to her, okay, you only have to use it once per day, Molly is probably going to continue using it three times per day. It's unlikely that she's actually going to revert back to once per day. And you have an ethical concern, even if you could achieve that, do you really want to take something that's working away from your little client, who's starting to do such a good job and gaining more control over her physiological state? You don't want to be that clinician who's taking away something that's working well. And then of course you have this overall scientific concern. If the client's behavior remains improved even though you're making these bi-directional changes then you really can no longer draw any causal inferences. And so I think that there are some behaviors where this might be a better treatment option, it's possible that my client with the crying maybe this would have been a good way to go, umm… because that felt a little bit less ethically iffy than something like helping a young child use the potty when the young child is four-years-old and has been having trouble. Umm… And so that doesn't feel right, but maybe there are other problems where this might be a little bit better and the clinical concerns aren't quite as severe. So, what did we talk about in this lecture? So, basically we talked about the fact that you don't have to be a formal treatment outcome researcher in order to take a scientific approach to doing treatment, right. You can be a full-time private practice clinician and still be a good scientist in the clinic. You can treat individuals and measure, you can do the simple approach which is to just treat your client and measure progress along the way and make changes when you see that progress is halted. Like… Like we saw with the umm… emotional processing therapy with the David client or you can have these more umm… more specific and detailed small-N experiments we call them, where clearly you only have one or four whatever clients, where you really are doing either the ABAB design or the multiple baseline design or the changing criterion design that allow you to make some causal inferences about the efficacy of your treatment. Keeping in mind that there are always going to be some realistic clinical concerns, some ethical concerns and some scientific concerns. All right, well, thank you for listening and I'd like to open it up for questions if anyone has anything that they'd like to bring up. 

00:44:20
UNKNOWN I had a question. If a clinician wanted to start using some of these more objective measures to see how their clients were progressing, but they hadn't used them before where might they go to become familiar with them or start selecting them? 

00:44:30
EVELYN BEHAR That's a great question. So in general, there… there are so many self-report questionnaires that are available, widely available to clinicians and they're in the public domain. So what I would do is umm… well, there are… there are two possibilities. Number 1, you can really just go online and do a search for measures of depression or measures of insomnia. And umm… very often those measures will be reprinted online. The other possibility is to do a literature search where you go to any treatment outcome. So, let's say that you want to do insomnia, that you're treating somebody with insomnia, you're doing a small-N design. You can go into and do a literature search on the most recent published treatment outcome studies on insomnia and then you go to the method section, and you look at the dependent variables, the dependent measures that were used and you might see something like the insomnia severity index or you know, something about cognitions and insomnia or whatever, and then you can actually do a search, an internet search on those specific names. And generally speaking, if they were used in a published treatment outcome study then they probably have very strong validity and reliability indices and so those might be safer to go with. And… And you also want to make sure that you're scoring it correctly because each different measures, upper part measure has its own scoring rules and so you just want to make sure that… that you're doing it correctly and there's tons of information available online for each one of those. Next. 

00:46:05
UNKNOWN Yes. You mentioned you had a client that had the crying issue and that there were no known therapy. So what does the clinician to do in those cases? 

00:46:15
EVELYN BEHAR That's a great question. So you know, the good thing is that in the world of psychology we have tons of research that tell us about umm… that… that tells us about what… what interventions we can use for different problems, but occasionally we do, we get somebody who just has a condition that has not been well researched. And I think this is where having knowledge about the nature of psychology, the nature of human beings, the nature of psychopathology, the nature of what happens when things go wrong, having knowledge about that is going to serve you very well. So for example, with that crying client, I had no idea what to do. I really, you know I… and I should say I tried relaxation first that didn't work, I tried all sorts of things, nothing was working and then I thought about literature from the social psychology world that looks at paradoxical interventions, and paradoxical intentions. And there's this great literature by Dan Wegner, when he was at the University of Virginia that looks at umm… when you try to stop something, like, if I say to you, whatever you do, don't think of a big white bear. Of course, of course you're going to think about the big white bear, you can't help it. And so there is this paradoxical effect of an intended action. And I thought, gosh, you know, maybe if I do a paradoxical intervention with her then it'll have the right effect, right. Like, if I tell her, okay, so try to cry, maybe then she'll be able to stop crying. Like if I say to you, okay, try to think of a big white bear, then maybe it becomes hard to think of the big white bear. So that's where I got the idea and I think the… the trick there is to have enough breath of knowledge in the world of psychology, in general that you can start to be creative and draw on different areas. Other questions. 

00:48:00
UNKNOWN I have a question. Do clinicians really do small-N experiments? 

00:48:05
EVELYN BEHAR That's a great question. Unfortunately, I think it's rare. Umm… And, you know, there… I think a couple of things are happening. Number 1, umm… the client has to be willing to do these things. And you know the client has to be willing to be better and then stop being better. And that often is not… it doesn't feel good to the client, it doesn't feel good to the clinician. Umm… However, it is done sometimes. And what's really nice is that very often clinicians who are in private practice will actually publish in peer reviewed journals the effects of a small-N experiment that they did on a particularly rare condition or difficult-to-treat condition. And it's an opportunity for practicing clinicians to… to publish which doesn't come along all the time. And secondly, it's so valuable to the practicing community and to the scientific community to know about those studies and to have them published because that's what gives us new ideas about how to treat people when nothing else is working. So to answer your question, I think it's rare, but when it's done, I think it's done well. And I would argue that more clinicians should use small-N designs. Okay, well, thank you so much for your attention and for your great questions. And I hope that you are walking away with a little bit more information about how to take a scientific approach to umm… to clinical work in a private practice setting umm… or really any setting outside of a university laboratory. Thank you 

00:49:45
Alexander Street 

00:49:45
This program has been made possible as a collaborative effort between Governors State University and the Alexander Street 

00:49:45
Executive Producers: 

00:49:45
Taney Shondel 

00:49:45
Shannon Dermer 

00:49:45
Presenter: 

00:49:45
Evelyn Behar, Ph.D. 

00:49:45
Associate Professor, University of Illinois 

00:49:45
Produced by: 

00:49:45
Governors State University 

00:49:45
Digital Learning and Media Design 

00:49:45
Department Director: 

00:49:45
Charles Nolley 

00:49:45
Video Producer/Director: 

00:49:45
Mark Kundla 

00:49:45
Video Editor: 

00:49:45
Mark Kundla 

00:49:45
Video Engineers: 

00:49:45
Heather Penn 

00:49:45
Arika Rogers 

00:49:45
Audio: 

00:49:45
Jack Mulder 

00:49:45
S. Patrick McCarthy 

00:49:45
Graphic Design: 

00:49:45
Amanda Zaija 

00:49:45
Camera Operators: 

00:49:45
Cherish Brown 

00:49:45
Levilyn Chriss 

00:49:45
Nikki Daily 

00:49:45
Kim Hudson 

00:49:45
Felice Kimbrew 

00:49:45
Jon Tullos 

00:49:45
Alexander Street 

00:49:45
© 2015 

00:50:10
END TRANSCRIPT 