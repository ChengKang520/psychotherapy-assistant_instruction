00:00:00
TRANSCRIPT OF VIDEO FILE: 

00:00:00
_____________________________________________________________________ 

00:00:00
BEGIN TRANSCRIPT: 

00:00:00
MICROTRAINING 

00:00:00
AN IMPRINT OF ALEXANDER STREET 

00:00:05
GREAT TEACHERS 

00:00:05
GREAT COURSES 

00:00:10
Types of Designs for Outcome Research in Counseling 

00:00:10
Presented By 

00:00:10
EVELYN BEHAR, Ph.D. 

00:00:10
UNIVERSITY OF ILLINOIS 

00:00:15
EVELYN BEHAR, Ph.D. 

00:00:15
ASSOCIATE PROFESSOR, UNIVERSITY OF ILLINOIS 

00:00:15
EVELYN BEHAR Hi, my name is Evelyn Behar. I'm an associate professor of psychology at the University of Illinois at Chicago. And I'm gonna be talking today about types of designs for outcome research in psychotherapy or in counseling. And this really, umm… comprises the majority of my research. I do research in two areas. Umm… I do research on anxiety disorders and I also do psychotherapy outcome research. And I sort of, combined these two things in order to, umm… find the answers to questions like which therapy should we use to treat a particular disorder or particular psychological condition. And I've been doing this type of research for about 17 years now. Uh… I'm really passionate about it and excited about it. And I'm excited to share some of that knowledge with you. Okay, umm… so we will go ahead and get started. So I'm gonna be talking today about types of des… designs for outcome research in counseling. And I want to start out by just talking for sort of, talking a big step back and talking about what the goal is of an experiment. So, umm… most people are familiar with the concept of an experiment where you randomly assign people to at least two conditions of the study. And that and you try to hold everything else constant so that whatever differences emerge between your two groups or your three groups or whatever you can actually, umm… attribute to that manipulation, right. So, the idea here is that we want to isolate the effects of the independent variable. And the more we can hold constant everything except that independent variable, the stronger our cause-and-effect conclusions are. And this, sort of, maps on to the idea of internal validity. So, internal validity is the extent to which you can be sure that your independent variable is the thing that caused a change in your dependent variable. And the independent variable is the thing that caused differences between your different conditions of the experiment. So again, just keep in mind that the thing you really want to do in an experiment is hold constant every single factor except that one little thing that you are manipulating. And in general there are, I mean, there are lots of different types of studies but there are, sort of, two types of studies primarily that people run. The first one is called a cross sectional study where you bring people into the lab and you do some sort of manipulation, you have your independent variable that you manipulate. And right there and then, you see how it affects your dependent variable and you draw your conclusions. The second type of study is called longitudinal study where you bring people in, you go ahead and you implement your intervention or your… your manipulation. And then you follow them over time and you might actually continue to implement that, umm… that manipulation over time. And you might follow them for a week, you might follow them for a few months, in some studies, you follow them for many, many years, umm… and those are called longitudinal studies. So, anytime you… and… and by the way psychotherapy outcome studies generally are longitudinal studies, right. Like you're not just brining people in and treating depression right now and then seeing if their depression is cured, it doesn't really work that way. You have to sort of follow people over time and then see if their depression is ameliorated over time. So anytime you do longitudinal research, there are some threats to internal validity. And remember, internal validity is the degree to which you can say that your independent variable was the thing that caused the change in your dependent variable. So, when you do longitudinal study, there are some threats to this that just naturally go along with that type of study. So some of these and I'll just sort of go through these briefly. Some of these are, umm… one is called history, so that is any event that occurs outside of the experiment that will cause your participants to change in some ways. So for example, if you… if you are treating PTSD over the course of six months and then war breaks out in your country, you have some terrible terrorist activity in your country, clearly that might actually impact levels of PTSD, right. And so there is this thing that's happening at a large scale that might impact the dependent variable. There is also maturation and this isn't… these aren't large scale changes, these are changes within the individual participant. So, here for example, if you have some sort of, umm… educational intervention where you are comparing different reading programs for children. And you want to follow them over the course of two years to see which program actually, umm… makes them better readers. Well, you have to keep in mind that these children are also maturing, as those two years go by. They are getting older and naturally they are gonna have better reading abilities. And so that's a threat to the internal validity of the study because of course there is this other thing that's specific to the participant that is naturally going to change them on that dependent variable. There is also something called repeated testing, so this is the possibility that being tested once might influence performance on later testing. So for example, if you want to, umm… assess the… the efficacy of different SAT prep courses or GRE prep courses, right. You are gonna have people take a pretest, they are gonna take an SAT. Then you're gonna randomly assign them, do a couple of different courses and then take a post test. Well, the fact that they already took that once, is necessarily going to impact, probably impact their performance on the later test. And there is another thing called instrumentation. And this is the idea that if you change your measures halfway through a study, umm… or part way through a study, then that is necessarily going to change the dependent variable. So, let's say that you have an outcome study, therapy outcome study looking at depression and you start out using the back depression inventory. And then halfway through, you say to yourself, you know what? I think the Hamilton depression inventory is better. I'm gonna switch to the Hamilton depression inventory. Well, are the changes in depression due to actual reductions in depression? Or they due to the fact that you now are using a different measure and you need to be aware of that. Additionally, there is something called statistical regression and umm… this is like in sports, when you have the… umm… the rookie of the year. And then the second year that the… that the person is an athlete, all of a sudden their performance really decreases. And everybody says, oh, it's the sophomore slump. It might not be the sophomore slump, it may just be this naturally occurring phenomenon that when you test somebody once and they score in an extreme way, either extremely high or extremely low, if you test them again, their score is gonna regress toward the mean. It's gonna become more normal, just naturally. And so it may be that someone's extreme depression score got better because their depression got better and the therapy is working. Or it may be that an extreme score in the beginning was then, when tested again, much less extreme which… which tends to happen. And then there is also something called selection bias, so these are differences in conditions resulting from different client characteristics in groups. So one of the reasons why we randomly assign people to conditions of an experiment is to try to make sure or to try to increase the likelihood that individual differences will get flushed out in that. And so you say to yourself, well, you know, by randomly assigning people to my two or three conditions are probably just given the laws of chance and probability, or probably end up with equivalent levels of baseline depression at the beginning, umm… equivalent levels of, umm… anxiety, equivalent levels of intelligence, equivalent levels of socioeconomic status. All of those things should get flushed out and they should end up being equivalent across your groups. But sometimes that doesn't happen. And sometimes you select your participants in such a way that somehow people, let's say with slightly higher levels of depression end up in one group versus the other and then that ends up being a threat to the internal validity of your study. And finally there is something called attrition, which is of course a dropout. Umm… You know, you bring in a hundred people to participate in your psychotherapy outcome study and some people are simply not gonna finish that study, they are not going to attend all 20 sessions or whatever. And if you have more dropout in one condition versus the other then that's called differential attrition and if you find differences between your groups at the end, like may be one group showed more improvement in depression relative to the other group, or it could be because that intervention was better, or it could be that you lost all the severe people, right. And there were more people dropping out in one condition versus the other or there's something that made them drop out and now that's showing up in the… in the outcome scores. So you got to keep all of these things in mind when you're running any kind of longitudinal study including psychotherapy outcome studies. Okay. So, shifting a little bit, let's talk for a moment about psychotherapy and psychotherapy outcome research. So we have to ask ourselves what's the purpose of psychotherapy outcome research. Well, clearly, it's so that we learn what will be the most effective intervention for a particular group of people or for a particular diagnosis, right. And… And as clinical psychologist or counselors or whatever mental health professional you are, you really do have an ethical obligation to provide an intervention that you… that there is good reason to believe, scientific reason to believe, is going to effective, right. So, for example, you know if you go to… if you go to a heart surgeon, you have heart disease and you really need heart surgery. You don't want the heart surgeon saying, you know, I'm not gonna read the science. I feel like this surgery is gonna be best for this person. You are gonna say, forget it, I'm not hiring you. I'm going to a surgeon who's gonna do the most "empirically" validated intervention for me, right. Because you want to make sure that the intervention that you are getting is the most effective intervention out there. Well, the same thing is true in psychology. The same thing is true in counseling. You want to go to a therapist who's gonna say, you know what? I'm not gonna go with my gut here. I'm gonna rely on the science and give this person, at least for starters, something that I know is, sort of, like evidence based medicine, right. Something that I really know, based on science, is likely to be effective for this person. So, then the question of course is, well, how do you figure out which type of therapy works best for particular condition. If you are the counselor or the psychologist, how do you know, how do you know whether to give behavior therapy or cognitive therapy or emotion focus therapy to your client with depression? And of course, the answer is well, you have to go to the literature and you have to consume the psychotherapy outcome research that can really inform this question. Okay, so let's talk a little bit about the elements that go into psychotherapy. And we can have lots of conversations about how many different elements going to psychotherapy, but in general, I think there are two categories of elements of psychotherapy. There's something that we call nonspecific variables or I tend to refer to them as common factors and a lot of people do. And then there is something called specific variables. So let's talk about the first one, let's talk about the common factors these nonspecific variables. It doesn't matter what kind of therapy you are using, right. Whether you are a behavior therapist, a cognitive therapist, a psychodynamic therapist, an interpersonal therapist, a psychiatrist, it doesn't matter. There are certain things that are always going to appear in the therapy, no matter what theoretical orientation you are coming from. Number one, you are paying attention to somebody's problem. If you are the client, you are having someone pay attention to what's wrong. Number two, you develop some sort of a therapeutic relationship which we know, umm… is really helpful to people. You have contact with hopefully a caring kind clinician, umm… who's paying attention to you. You, in almost all forms of psychotherapy, there's some form of emotional arousal that occurs, right. People go to therapy, they cry, they get angry, they feel hope, there's some sort of emotional arousal that's taking place. Generally, there is a better understanding of your problem, right, because there is the therapist who's sort of conceptualizing the problem for you. Umm… Whether it's talking about what happened in your childhood or what happened… what's happening now or what your thoughts are, your relationships, there is some way that you are better understanding what's going on. There is some expectancy and credibility, so you believe that something is going to get better. There is hope and faith. There are suggestion effects, right. So, as a therapist you might say, look, I think that you are gonna get better. And then, voila, the person gets better, sort of, like a placebo effect. Umm… There is a real opportunity for success experiences and there are demand characteristics. Demand characteristics are the idea that, you know, this person has been spending so much time and energy helping me, I better report that I'm getting better, right. And there is… there is almost that… that sense of demand. Okay, those are the nonspecific variables. Those are always there in any kind of psychotherapy, pretty much. And then there are the specific variables. So, these will… will really vary by treatment modality. In cognitive therapy, the specific intervention is to challenge client's automatic thoughts and schemas about the self, the world, and the future. In behavior therapy, you are encouraging the client to approach scary things, to behave in new ways, to try new things. In interpersonal therapy, you are teaching the client how to have more adaptive and effective ways of relating to other people. And in general, I would argue that it's very important as a clinician to make sure that whatever change we see resulting in a client, is resulting not only from the nonspecific factors because we didn't go into this just to be a nice person. But rather from the specific factors that I have something unique that I can offer you that your sweet granny can't, right. Because you can always go to your granny's kitchen and sit, and get all those nonspecific factors. But your granny probably can't do cognitive restructure, maybe she can. But in general, she probably can't do it like a pro. Okay. So, let's… Now with all of that information in the background, let's talk a little bit about the different types of treatment outcome design. So I have them listed here. And what I'm gonna do is, I'm gonna walk you through them. And I'd like you to keep in mind that as we go through them, up until the comparative design, they become increasingly stronger in internal validity. So they really, they start… it starts out with a no-treatment comparison design where there is not their… the strength of the causal conclusions is not very high. All the way up to the dismantling, additive, catalytic and parametric designs, where the strength of causal conclusions can actually be very strong, very high. Okay, so let's get started. First is the no-treatment or the waiting list comparison design. So here you randomly assign your participants to either receive treatment, and in this case, I'm just gonna use just to… just to make things simple, cognitive behavior therapy. So either you… you randomly assign people, either to get cognitive behavior therapy or to nothing. And it's considered unethical to really just give them nothing. So instead you want to put them on a wait list. And when these folks, when the treatment group is done with treatment, then you transition the waiting list group into treatment so that at least you are not withholding treatment from a group of people, okay. So you assess them at pre-therapy, you go ahead you do your intervention and then you assess them at post-therapy. Okay, so and probably what's going to happen is that you are gonna find that your treatment group is superior, that the outcome, right, is superior to the wait list group. Why? Because doing something is 99% of the time better than doing nothing. So, what is this control for? Now thinking back to internal validity. Well, it controls for all variables related to the passage of time, right, because both groups are going through the same exact passage of time. So things like history, maturation, statistical regression, all of that stuff, but that's it. So, when we conclude that these two groups are different, what are our causal conclusions? Well, what we can say is that, we know that there's something about CBT, that's better than doing nothing, we know that. But we don't know what it is about CBT and by the way it could just be those nonspecific factors, right. Because we didn't control for nonspecific factors there, we didn't control for those common factors. So we can't say, hey, CBT has active ingredients. We can't say that because we do not control for something that we know makes people better and that's those common factors. So here are the pros of the wait list comparison design. Number one, it's cheap to run. Generally speaking, they're… these are not the most expensive designs, the most expensive experiments to run. And you generally get very large effects because again doing something is usually better than doing nothing. So you get to publish the paper and say, look, there is this huge difference between these two groups. But you didn't really learn that much about the nature of CBT or the nature of anything. The cons are that that it doesn't control for this "placebo effect," sort of, that hope and expectancy, and credibility, and all of that stuff. There is no follow up measurements. So you can't follow people a year later, because remember, you took that wait list group and you gave them treatment, right. And so now, you can no longer compare a year later how those two are doing because now everybody got treatment. So, it maybe that CBT has no lasting effects and you will never know it. Umm… Also, you need a way of monitoring the people in your wait list comparison condition. Because if some people, for example, get worse or become suicidal, you have an ethical obligation to intervene. So, you are going to be monitoring them, which is not the same as not getting treatment, they are being paid attention to. And we know that when people are paid attention to, they tend to get better. Umm… There is also the risk of a selection bias, so the client who agreed to delay treatment, the people who end up in that wait list comparison, they may have milder symptoms. May be the people who say, you know what, if I'm gonna be in the wait list, I don't want to be in your study. Those may be the more severe people and tonight you might end up with a treatment group that's more severe at baseline, relative to the wait list group. And there's an ethical issue here. You have a really good reason to believe that CBT is gonna work and you are making people wait for it. And I think that that's a real ethical dilemma. And I don't know that the strength of causal conclusions is really worth that ethical dilemma, making people wait for example four months, while the other people get treatment, okay. So there are lots of people who say, you know what, this should never be done. We already know that treatments work better than nothing and let's just retire the no-treatment comparison designs. It's not worth it, it's not worth those cons. Okay, so let's step it up a tiny bit. Let's look at the common factors comparison design. So here, again, you're gonna randomly assign people to one of two groups, the treatment group will get CBT. And remember, that CBT already inherently includes those nonspecific elements, right, it includes common factors. And then your… umm… your other group, your control group, will only get common factors. What does that look like? That looks like Rogerian therapy, right. Where the person gets unconditional positive regard, the therapist says a lot of things like that sounds so hard, tell me a little bit more about that. So why did you do then? Well, now what you are gonna do? But there's no active intervention, there're no specific elements of treatment. You don't try to get them to change their behavior, you don't challenge them, you don't do anything like that. You keep it at the level of Carl Rogers, which by the way is lovely. And it really does help people. And so it's a stronger, notice that it's a more conservative test, right. It's a stronger test. So, what is it controlling for? It's controlling for the passage of time, just like the other one did. And its controlling for common factors of therapy. So, when you find, and you probably will, when you find that your two groups are significantly different at post-therapy, you can say, look, there is something about CBT that is cause… causative of change, it's causing people to get better. And it's not just the common factors, it's not just that they are being paid attention to, that I'm giving them hope and expectancy incredibility, that I'm being really nice to them, that I'm giving them all of this support, that's not all it is. There is something unique, there is something specific in CBT that's helping them, okay. Now what we don't know is what it is about CBT. Is it the "C," is it the cognitive stuff? It is the "B," is it the behavioral stuff. Do we need to get the snake-phobic to actually interact with the snake or do we not need to? Can we just challenge their cognitions? This design doesn't tell you anything like that. All it tells you is that there is something about that CBT that's specific, but we don't know what. It's almost like, you know, when you look at a bottle of Tylenol or any medication and it has the active ingredients and the inactive ingredients, right. If there are six active ingredients, there are six things in that medication that are supposed to have causative effects and then there is like the sugar, you know, that makes up the actual pill. Don't you want to know if there are six things that make up the actual medication? Do I need all six? Or can I get rid of some? This is the… This is an equivalent question, you know. Do I need the "C" and the "B"? Do I need all of these different elements of CBT? Can I get rid of some? This design doesn't answer that question. And so again, there are some people who say, you know what, nice design, better than weightless comparison, not enough. We should retire this one too. Let's step it up and let's reach stronger causal conclusions. Okay, so now let's step it up. Oh, sorry, the pros. It's better ethically than the weightless comparison, right. You are not making anybody wait. Umm… And clients are less likely to decline receiving treatment, so you are less likely to end up with that selection bias that we talked about in the weightless comparison. The cons are, again, ethically, you are knowingly denying clients a treatment that you suspect will be better. You suspect that CBT will be better otherwise you wouldn't be testing it probably. And there might be different levels of credibility and expectancy. So when you present that cognitive behavioral rationale to the clients, it is very believable. But when you present the rationale for supportive listening, for that Rogerian therapy, it's less believable, right. Like, well, we find that when people have a place to go to talk about things they end up feeling better, they are gonna believe in that, but maybe not as strongly as they believe in the… in the CBT rationale. And so it might not be the treatments that are leading to differential effects, but rather that placebo effect, that expectancy and credibility that they will get better. And again, you are still not able to say what it is about CBT specifically that's leading to change. Okay, so now let's step it up to a design that is, I think much, much stronger than the other two. Okay, this is called the dismantling design. So here you are gonna randomly assign people to three different conditions. The first one is gonna get the full treatment package. And again, I'm gonna stick with cognitive behavior therapy just to make this easy. It's got cognitive therapy and behavior therapy. You assess them at pre-therapy. You assess them at post-therapy and follow up. In the second group, they get cognitive therapy alone, no behavior therapy. And in the third group, they get behavior therapy alone, no cognitive therapy. And so what you are doing, you are dismantling this design so that you can see well, which elements are necessary and which ones can I toss, right. Which are the active ingredients in that… in that Tylenol bottle? Again, it controls for the passage of time, it controls for all the nonspecific factors because nonspecific factors are going into all three of these. And when the causal conclusions here are much stronger, so notice, if you find that group one does better than the other two, then you know I got to include both of these things, right. If I toss one, efficacy decreases. So I've got to include them both. Now let's say that only cognitive therapy and the combined condition, right, that groups one and two are better than group three. Then you know that you can get rid of behavior therapy, right. You get rid of it, these are equivalent, they are both superior, but they are equivalent to each other so why do I need behavior therapy, I can just go ahead and toss behavior therapy, just stick to cognitive therapy. I'm gonna save my clients a lot of time and money. And now I'm getting it down to that active ingredient in that medication bottle that I know is actually causing change, okay. All right, now, a methodological consideration in the dismantling design. Notice that here you've got cognitive therapy plus behavior therapy. Here you have cognitive therapy alone and here you have behavior therapy alone. If each one of these is 30 minutes long, right. Your group two, well, you want to give them 30 minutes because you don't want to give them a double dose of cognitive therapy otherwise it's not a fair comparison. So, if you only give them 30 minutes, well, what about this? These guys are getting a full hour of therapy. These guys are only getting 30 minutes. And so you have a little bit of, like, a design dilemma. And same thing here, these guys are only getting 30 minutes because you don't want to give them a double dose, right, otherwise it's not a fair comparison. But now they are only getting 30 minutes worth of therapy. So, this is what is referred to as session parameters. And what you want to do is you want to make sure that all of your groups have equivalent session parameters. So what you are gonna do, you are gonna give 30 minutes of cognitive therapy, 30 minutes of behavior therapy, that group is easy. Then for these guys, you are gonna do 30 minutes of cognitive therapy and then 30 minutes of common factors where you fill the time, it's a filler. And you say, okay, now for 30 minutes you get to talk about whatever you want and you play Carl Rogers. Same thing here, you do the 30 minutes of behavior therapy and the other one you fill(ph). Okay, in that way, everybody is getting 60 minutes. And you've kept the dose of each individual component equivalent across the three conditions. It's a nice… It's a nice little, uh… It's a nice fix for a little bit of methodological dilemma. Okay, next the additive design. So here, let's say that you figure out, you know what, I need both cognitive therapy and behavior therapy. It's the full CBT package it's what's important but gosh, only 50% of my clients are getting better, the other 50% aren't getting better. So clearly, CBT is good for about half of people. But I'm gonna try to boost its efficacy a little bit. What can I do? How can I boost its efficacy? Well, then you can do an additive design. So this is where you are trying to boost the efficacy of a particular treatment. So you randomly assign people, let's just say you think to yourself, well, you know what, interpersonal therapy has some nice empirical support. Let me see if adding cognitive behavior therapy plus interpersonal therapy will be better for people, right. So you randomly assign people either to get CBT plus interpersonal therapy versus CBT alone, right. And again, you are gonna control for those common factors by adding in, right, so each one of these is gonna get, let's say an hour of CBT, these guys are getting an additional hour of interpersonal therapy. So now you got to fill that extra hour for the second group. So you will give them that Carl Rogers intervention where you're really just doing those nonspecifics those… those common factors of therapy. Okay, so here is an example, umm… from the literature. Actually, this is done by, umm… Stewart Agras at, uh… Stanford University. So, umm… back in 1989, uh… they started… they were thinking about treating bulimia. And trying to find a better way to treat bulimia because they knew that CBT worked okay, but it really wasn't getting people much, much better. And they thought, you know, what if the purging behavior in bulimia is an anxiety reduction technique, right. Like people are really anxious, they just ate all their stuff and now if they just purge it by vomiting or exercising or whatever, then their anxiety will decrease. And how do we treat anxiety in general? Well, we expose people to what they're afraid of. And then we do response prevention. We don't let them escape. We don't let them indulge in the thing that makes their anxiety artificially decrease. So he thought, you know what, let's try it for bulimia. Let's randomly assign people to either CBT plus response prevention, where they would have people actually eat a lot and then not purge until the anxiety naturally decreased or CBT plus common factors, okay. So if… if the… I forget what exactly they… how much time was allotted, but if the response prevention here was say 45 minutes then they would fill it with 45 minutes of common factors. And, umm… okay, so if the CBT plus response prevention group were superior to CBT alone then we would know, listen, you got to include response prevention, it leads to superior efficacy, right. So now that should go into treatment packages for bulimia. If they were equivalents then, you know, get rid of response prevention, you don't need it. It's not causally adding anything. It's not causing enhanced change in this group, so clearly, that's not the answer, in terms of boosting efficacy for treatment of bulimia. Okay, moving along, there is another thing called the catalytic design. So this is the idea that one element of therapy might actually have a potentiating effect on another element of therapy. And I'll give you a real example from a study by Tom Brokovich(ph), who is my Ph.D. advisor. So we know that there are two umm… elements of therapy that can be, uh… effective for specific phobias. You can do relaxation training that tends to help people. And we know that exposure therapy definitely helps people. Now what if doing relaxation before exposure, somehow makes exposure more efficacious, right. So, it's almost like well, can… can relaxation potentiate exposure therapy? Can it catalyze exposure therapy? So you can randomly assign people to two groups. You can either do relaxation plus exposure or the flip, exposure plus relaxation, right. And that's the order in which those elements are presented. So if you see that group one is better than group two then you know, gosh, there is something about having this order that is potentiating change. That's a great design and you see that everything here is health constant, right. There is no difference. The only thing that changes is the order of presentation of those treatment elements. And then there is the parametric design. So this question, once you know that a particular element of therapy is effective. For example, you know exposure is effective for anxiety treatment, that's easy. Well, how much exposure? What… What quantitative changes can we make to the treatment to increase its effectiveness? That's basically the question that's getting asked. So for example, in cognitive therapy, you might ask, you know what, if I'm challenging people's cognitions and their automatic thoughts how assertive should I be? How much should I really needle them? How much should I just be after them to really challenge, and challenge, and challenge? Should I do it at a low level? Should I do it at a medium level? Should I really be assertive and do it at a high level and not let it go until I see that there is cognitive change? Well, that's a parametric question and you can randomly assign people to those three different conditions. Same thing with systemic desensitization or exposure. What length of exposure should I use for treatment of spider phobias, right. Should I do 30 minutes? Should I do 60 minutes? Should I do 90 minutes? Should I do 10, 20, 30? What… What umm… What quantitative change can I make to make sure that this is as effective as possible? And then keep in mind again the only thing that differs between these conditions is the dimensional level of that single intervention, that single technique. So again you are holding everything constant except that one dimension. And what's nice here is that you can find nonlinear trends, right. So may be doing exposure for 30 minutes, for 20 minutes is great. But doing exposure for 10 or 30 is not so good until you can find that nonlinear effect. Okay, so those are the, umm… those are the treatment outcome studies going from, sort of the… the least, uh… controlled, all the way to those four, the, umm… dismantling, additive, catalytic and parametric that are quite controlled. And then we have the design that is probably done in about 90% of treatment outcome studies and this is worrisome to me. So, umm… I'll present it and you're gonna say to yourself in the beginning, wait a second, what do you mean that's a problematic design? Of course, that's a good design, but bear with me, let's see if I can convince you. Okay, so here is the comparative design. So up until now, we have talked about the importance of holding constant all factors except the one factor that you are interested in, right, that independent variable. So in the wait list design, we were holding constant the passage of time variables. In the common factors design, we held constant that placebo effect and also the passage of time variables. In the dismantle… dismantling and additive designs, we held everything constant. In the catalytic design, we held everything constant. In the parametric design, we held everything constant. And we talked about how these four designs really, umm… are associated with the strongest cause of conclusions, right, and these, less so. Okay, so now the comparative design. Society understandably has a burning question, which type of therapy is best, right. If I have a client who comes in and that client is severely depressed, should I do cognitive behavioral therapy? Should I do psychodynamic therapy? Should I do interpersonal therapy? Should I put the client on medication? What should I do? I don't know and it's not intuitive, right. And everybody, sort of, has their individual preference and everybody says, well, I like doing this one or this is how I was trained, but that doesn't matter as much as what actually works. And most psychotherapy outcome studies will pit these treatments against each other. And… And it's because there is this demand from society and from treatment providers saying, you got to tell us what works best. We got to know so that we can deliver it. So they might compare CBT versus interpersonal therapy. There have been lots of studies like that. CBT versus medication, there are countless studies looking at that. Cognitive therapy versus behavior therapy, lots of those. Cognitive therapy versus medication, lots of those. And the list goes on and on and on. I'm sure we can all think of lots of different examples of these comparative designs. Where you really… And the thing here is you're comparing different traditions to each other, that's really… you are comparing different theoretical orientations to each other. So, let's say that you run a very big comparative design, where you are comparing for depression, cognitive therapy versus behavior therapy versus interpersonal therapy versus medication. What's being held constant here? Well, not much, right. There is… There is actually, there are a million different things that go into cognitive therapy, a million different things that go into behavior therapy, tons that goes in the interpersonal therapy and lot that goes into medication. And when you think about this idea of holding constant lots of factors except that one thing that you are interested in, all of the sudden, this doesn't seem to approximate that so closely. So what varies across the conditions? Well, first of all, every single technique employed, every single one, perhaps, the quality of treatment, which by the way, we are not good at measuring, right. So may be, the interpersonal therapy is being delivered expertly and the behavior therapy is really being delivered at a very amateur level. Umm… The expectancy and credibility of the clients might differ, right. May be there is just something about interpersonal therapy that is deeply believable to people. And something about cognitive therapy that isn't, like, what, if I change my thinking, I'll feel better? Come on, that's not gonna work, right. Therapist bias might differ. So you might have your interpersonal therapist really believing in what they are doing. And your cognitive therapist really not believing so strongly in what they are doing. Amount of time at the clinician, think about medication management, right. You get 15 minutes with the prescribing physician and that's it. You don't get an hour, two hours, that doesn't exist in the world of medication management. The type of clinician differs, there're so many things that differ across all of these conditions that if and when you find differences across your conditions, you have no idea what caused them. You really, you cannot pin-point the causal element. And so you can't pinpoint causal mechanisms of those treatments. It's almost like saying I have two pills, this pill has five really, umm… potentially active ingredients. And this pill has a different five active ingredients. If you compare them, well, you might find that this pill is better, but why? Was it the first ingredient, the second ingredient, the third ingredient? It doesn't tell you what caused it to be better, it just tells you that it was better, that was it. Now in psychotherapy it's more complicated than the pill, uh… than the pill comparison. Because in psychotherapy, each one of those ingredients might bring in different demand characteristics, might bring in different credibility and hope and expectancy. It might bring in, umm… different, uh… a different type of relationship with the clinician. And the more you vary all of those things across all of those different treatment components, all of those different conditions, the less control you have. And then when you find differences you say to yourself, oh, what caused it? Was it a particular element? Was it different therapeutic relationship? Was it different credibility in expectancy? I really have no idea. And so I would argue that this is actually not a very strong design. And I understand why so many people do it. Because there is this… there is this demand from society. But I would say, retire it, you're better off going with the dismantling the additive that will give you strong causal conclusions. So if we want to answer questions about relative efficacy, for example, we can do an additive design. I can do CBT plus interpersonal therapy versus CBT plus common factors. And if my first condition is superior, I know that interpersonal therapy has some additive value. It has some incremental value. There is something about that therapy that is really good and that adds something. I don't need to pit the two against each other and come up with no causal conclusion, or no strong causal conclusions. Okay, so, umm… basically, just to… just to wrap it up, we talked a little bit about, umm… what goes into a strong experiment. What some of the threats to internal validity are, this idea that you want to hold constant everything except that one thing that you are manipulating. And in longitudinal data, what can threaten some of those conclusions. And then we talked about seven different designs. So we talked about the wait list comparison and the no… and the common factors comparison. Wait list comparison, really not very strong conclusions at all. Common… Common factors comparison, a little bit better, but still not very strong. We talked about those four, kind of, golden, uh… designs, the dismantling design, the additive design, the catalytic design and the parametric design. We've got lots of experimental control between all of those, umm… across all of those conditions and really allow those… those treatment outcome designs allow you to make very strong causal conclusions. And then we talked about, unfortunately, the most common treatment outcome design of all which is the comparative design. And we talked about, umm… you know, what some of the pitfalls there might be and why the conclusions from those studies may not really be all that strong. Okay, so I think we can go ahead and open it up for discussion and I would love to hear any questions if you have them. 

00:40:30
UNKNOWN I had one. Can you help us understand how you ensure consistency of therapist approach either, you know, within it within one condition or across conditions? 

00:40:40
EVELYN BEHAR That's a great question. Umm… There's actually another video that's being done, uh… it's called understanding psychotherapy outcome research, where I'll be talking a lot about, umm… some of these very specific things that go into psychotherapy outcome research and all of these methodological considerations that you have to take into consideration, umm… when you are designing them. But in general, there… you know, you are right, like when you are randomly assigning participants to different conditions of your study, you've got to make sure that there are lots of… there are so many different things that you have to ensure are consistent across those conditions. So for example, umm… there are client considerations that you have to really consider, umm… where you… for example, you want to make sure that, umm… the severity and duration of the problem is equivalent across your conditions of the study, right. So you don't want one condition, umm… where the clients show much higher severity or duration of the depression or anxiety or whatever relative to the other ones. And as you brought up, there are therapist considerations. So for example, you know, you want to make sure that when the therapist is doing cognitive therapy versus behavior therapy that there are lots of things that are equivalent so that really, the, you know, the only thing that is different is the actual delivery. So, umm… you might, for example, want to have an adherence checker, who listens to the tapes of the therapy sessions. And knows that here she is supposed to check off, for all of these different elements of therapy that the therapist either did or forgot to do. And then you want to have some sort of a cut off and say look, if the therapist makes two mistakes, we are gonna omit this person's data because now our… you know, our… our… umm… internal validity is compromised because that… that delivery of treatment is not what we intended. So there are lots and lots of ways in which you need to make sure that, umm… elements that different conditions are equivalent on, you know, we say, it's like infinity minus one variables, right. There is a finite number, but it's really, really high. And so we try to think of these things in advance and just try to make sure that different conditions are equivalent. Sure, other questions? 

00:43:05
UNKNOWN I have a question. Umm… You mentioned earlier that 90% of the studies that are done are comparative in nature. 

00:43:10
EVELYN BEHAR Yeah. 

00:43:10
UNKNOWN And you are saying, it's not really effective, 'cause it doesn't hold for that consistency. Is there a reason why they don't do the additive or the… the other types of studies? Is it easier? 

00:43:20
EVELYN BEHAR Yeah. It's a great question and, you know, I don't know that I have an easy answer because I ask myself this question all the time. Like if the… if the internal validity is low then why is that… why does this comprise in 90% of the studies that we see and why aren't people just moving away from the comparative design in favor of the other one? So I think there are… there are a couple of answers. Number one, I don't know that there are a lot of treatment outcome researchers who are aware of the pitfalls of the comparative design. And, you know, I was fortunate to work with, umm… a Ph.D. mentor who was a treatment outcome researcher and who really was the first person to, umm… be very skeptical of the comparative design. And he has written about it, I've written about it, but that doesn't mean that's it part of everyday knowledge for researchers. And the second thing is, as with many things, umm… the… there is a compulsion to satisfy an audience as opposed to really have scientific integrity, right. So a lot of times people see the problem, but they say, gosh, but, you know, the National Institute of Mental Health is funding research like this and I need grant money. And so I'm gonna go ahead and I'm just gonna propose running this type of study because I want the $5 million to run the study even though I know that may be, scientifically it's not the strongest study. And, you know, when there is this draw of, umm… society's burning question and the desire to answer that question for society, which I think is coming from a very good place. The desire to fund your own laboratory so that you can have students and support staff and all that, that's coming from a good place. Umm… But I think sometimes that issue of the scientific rigor, sometimes gets put aside, in favor of the other staff. And I would just really, umm… encourage people to try to maximize both things. You know, maximize answering society's questions and getting good funding, but also stay true to scientific rigor and methodological sophistication. 

00:45:30
UNKNOWN Thank you. 

00:45:30
EVELYN BEHAR Sure. Other questions? 

00:45:35
UNKNOWN I had one. 

00:45:35
EVELYN BEHAR Uh-huh. 

00:45:35
UNKNOWN I see that you use this, a lot with your work with CBT. And CBT is a more structured theory. 

00:45:40
EVELYN BEHAR Yeah. 

00:45:45
UNKNOWN What… CBT is a more structured theory, can the… can this be applied to… to theories that are not are structured? 

00:45:50
EVELYN BEHAR This is a great question and I think this is probably the top, umm… potential criticism of, sort of taking these highly structured approaches. So as you are saying, you know, cognitive-behavior therapy it's in the name, it's a hyphenated name. You have the cognitive therapy and you have the behavioral therapy. And you're putting them together and so it's very easy to split them apart. And for example, do a dismantling design, right. But what about something like psychoanalysis? You know, we… a lot of people criticize psychoanalysts for practicing an approach that doesn't have a lot of empirical support behind it. But to be fair, psychoanalysis is not so easy to dismantle, right. There are probably hundreds of elements and techniques that go into psychoanalysis and it takes multiple years often. And by definition, it's a little bit hard to, sort of, bring structure into that and say, okay, now I'm gonna write a treatment manual, on how to do psychoanalysis. Those manuals don't exist. You just kind of learn it by being a trainee. And now I'm gonna dismantle in this nice clean way. And, you know what, it's not so easy to do a psychoanalysis. There are other treatments, like for example, in a personal therapy that have been manualized and they have sort of tried to identify the, umm… the top elements of interpersonal therapy, so that it can be subjected to scientific enquiry a little bit better, umm… or things like emotion focus therapy are also little more structurable. But your point is very well taken that there are… not every intervention is adaptable to something like the dismantling design or even to being, umm… put into a manual, which you need in order to train therapists. And in order to make sure that they are doing what they are supposed to do. So that's a, it's a great criticism and a great question to ask and, uh… yeah, it's… it's well taken. Thank you for asking it. Any other questions? 

00:47:55
UNKNOWN I had one other question. It seems as though under umm… these designs that you, sort of, assume not only that any approach can be broken down to parts, but that any two approaches can be added together. Have you found that there are certain approaches that don't logically or theoretically link well to each other? 

00:48:15
EVELYN BEHAR That is also a great question. Umm… yes, something like, for example, if you were to do an additive design, something like cognitive therapy where you're really, umm… challenging and trying to change people's thought patterns and schemas, added with something like acceptance and commitment therapy, where it's a lot of focus on, you know, just, kind of, like, accepting that this is the way that it is for you right now. Umm… May be you are committing to changing this little thing or that little thing, but you are not… you are never labeled pathological in any way. Nothing is ever pathological, whereas in cognitive therapy, you are being told, these are dysfunctional attitudes. These are dysfunctional thoughts, right. And by definition that sounds pathological, I mean, it's being defined as a pathology for the client. So how are you supposed to do an additive design here, where in one hour you are saying, okay, now let's fix your pathology. And in the other hour, you are saying, listen, you can't pathologize(ph) yourself. It's these two things really seem inconsistent. So, uh… it's another, you know, that's a… that's a… that's a potential criticism that's also very well taken that not all theoretical orientations are combinable to look at something additively. For example, umm… and… yeah, I, I guess, you know, my… my answer to that would be, some additive designs maybe shouldn't be done. And, umm… or may be those treatments, the rationale for those treatments can be tweaked a little bit, to make them seem more compatible, make them seem not so logically inconsistent with each other. And to sell it to the client because, you know, the rationale is very important for the client. You have to be able to present a credible rationale, otherwise you've lost them. So if you can present both things in a way that is logically consistent then maybe there is a way to look at these things additively. Umm… You know, especially if it's a good scientific question and there is a real reason to believe that these two things might combine to lead to enhance change then I would say, you know, there is got to be a way to work with them together in order to sell them in a… in a credible way without… without stretching the truth. I mean, like genuinely sell them in a credible way. Thank you. 

00:50:40
Alexander Street 

00:50:40
This program has been made possible as a collaborative effort between Governors State University and the Alexander Street 

00:50:40
Executive Producers: 

00:50:40
Taney Shondel 

00:50:40
Shannon Dermer 

00:50:40
Presenter: 

00:50:40
Evelyn Behar, Ph.D. 

00:50:40
Associate Professor, University of Illinois 

00:50:40
Produced by: 

00:50:40
Governors State University 

00:50:40
Digital Learning and Media Desig 

00:50:40
Department Director: 

00:50:40
Charles Nolley 

00:50:40
Video Producer/Director: 

00:50:40
Mark Kundla 

00:50:40
Video Editor: 

00:50:40
Mark Kundla 

00:50:40
Video Engineers: 

00:50:40
Heather Penn 

00:50:40
Arika Rogers 

00:50:40
Audio: 

00:50:40
Jack Mulder 

00:50:40
S. Patrick McCarthy 

00:50:40
Graphic Design: 

00:50:40
Amanda Zaija 

00:50:40
Camera Operations: 

00:50:40
Cherish Brown 

00:50:40
Levilyn Chriss 

00:50:40
Nikki Daily 

00:50:40
Kim Hudson 

00:50:40
Felice Kimbrew 

00:50:40
Jon Tullos 

00:50:40
Alexander Street 

00:50:40
© 2015 

00:51:05
END TRANSCRIPT 